# Daily PySpark Challenges

this repo is a personal collection of **daily pyspark challenges** that simulate real-world data engineering problems.  
each problem is designed to strengthen practical spark skills â€” covering data cleaning, joins, aggregations, window functions, optimization, and sql-parallel logic.

## Purpose

the goal is to build hands-on understanding of pyspark through consistent problem-solving, not just theory.  
this repo acts as a practice log, a reference for common spark patterns, and a showcase of structured, production-style thinking.

## whatâ€™s inside

each folder represents a separate challenge and usually contains:
- **PySpark_Solution.ipynb** â€“ pyspark implementation of the challenge  
- **SQL_DataSet.sql** â€“ sample dataset used for testing  
- **SQL_Solution.sql** â€“ sql equivalent for quick validation or comparison  
- **Expected_output.png** â€“ expected final result for reference  

example folder:
```
ğŸ“‚ Problem 3 - Identify First-Time and Repeat Customers by Date/
 â”£ ğŸ“œ PySpark_Solution.ipynb
 â”£ ğŸ“œ SQL_DataSet.sql
 â”£ ğŸ“œ SQL_Solution.sql
 â”— ğŸ–¼ï¸ Expected_output.png
```

## Learning focus

- building scalable transformations in pyspark  
- comparing pyspark vs sql logic for better understanding  
- improving debugging and optimization habits  
- developing clean and reusable data workflows  

## Tech used

- apache spark (pyspark)  
- sql (postgres compatible)  
- jupyter notebook  
- parquet/csv datasets  

## How to use

```bash
# clone the repo
git clone https://github.com/Moha-Azar/Daily-PySpark-Challenges.git
cd Daily-PySpark-Challenges

# open a challenge notebook
jupyter notebook "Problem 3 - Identify First-Time and Repeat Customers by Date/PySpark_Solution.ipynb"
```

## Progress

problems are added regularly â€” from basic to advanced â€” to cover a wide range of data engineering use cases.

## Note

these are self-built challenges for continuous learning.  
each solution is tested locally on pyspark and sql to ensure correctness and clarity.
